{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyun1006/deeplearning-pytorch/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**2. Word2Vec**\r\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\r\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\r\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818b6b26-d3f4-462e-adb1-c4ff5924fb35"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 12.8MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from konlpy.tag import Okt\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import torch\r\n",
        "import copy\r\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\r\n",
        "\r\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  \r\n",
        "학습 데이터는 1번 실습과 동일하고, 테스트를 위한 단어를 아래와 같이 가정해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\r\n",
        "  \"정말 맛있습니다. 추천합니다.\",\r\n",
        "  \"기대했던 것보단 별로였네요.\",\r\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n",
        "]\r\n",
        "\r\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt() #Openkoreatext"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\r\n",
        "  tokenized = []\r\n",
        "  for sent in tqdm(data):\r\n",
        "    tokens = tokenizer.morphs(sent, stem=True)  #morphs 형태소 추출\r\n",
        "    tokenized.append(tokens)\r\n",
        "\r\n",
        "  return tokenized"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711ccd83-ff90-4207-caba-44f6bd66cced"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)\n",
        "print(train_tokenized)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a59be4b-4b58-46b2-a96b-03668937d258"
      },
      "source": [
        "word_count = defaultdict(int)\r\n",
        "\r\n",
        "for tokens in tqdm(train_tokenized):\r\n",
        "  for token in tokens:\r\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 9670.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb7ad7c-9374-4922-af93-f410174984a4"
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n",
        "print(list(word_count))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26c3701-0331-4541-8e97-0410a9e80ecb"
      },
      "source": [
        "w2i = {}\r\n",
        "for pair in tqdm(word_count):\r\n",
        "  if pair[0] not in w2i:\r\n",
        "    w2i[pair[0]] = len(w2i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 102051.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1611bbdb-e556-49c5-d3ce-24cf82a9f698"
      },
      "source": [
        "print(train_tokenized)\r\n",
        "print(w2i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "class CBOWDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids): \r\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.y.append(id)\r\n",
        "\r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.x += [id] * 2 * window_size\r\n",
        "          \r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51ee6dd-7669-49d5-a054-8b00bb3f6cb5"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\r\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\r\n",
        "print(list(skipgram_set))\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1967.13it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 34351.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \r\n",
        "\r\n",
        "\r\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(CBOW, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x):  # x: (B, 2W)\r\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w) 왜 평균을 안 구하는지\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(SkipGram, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x): # x: (B)\r\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\r\n",
        "learning_rate = 5e-4\r\n",
        "num_epochs = 5\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\r\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b54b55-6a11-4170-927b-21ee98bbd60f"
      },
      "source": [
        "cbow.train()\r\n",
        "cbow = cbow.to(device)\r\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(cbow_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = cbow(x)  # (B, V)\r\n",
        " \r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 86.64it/s]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 4.361729621887207\n",
            "Train loss: 5.553689002990723\n",
            "Train loss: 4.872152328491211\n",
            "Train loss: 5.099614143371582\n",
            "Train loss: 4.8152923583984375\n",
            "Train loss: 5.830530643463135\n",
            "Train loss: 5.213549613952637\n",
            "Train loss: 5.005040645599365\n",
            "Train loss: 4.352278709411621\n",
            "Train loss: 6.059813499450684\n",
            "Train loss: 5.066593170166016\n",
            "Train loss: 5.553886413574219\n",
            "Train loss: 5.1624579429626465\n",
            "Train loss: 4.621767520904541\n",
            "Train loss: 5.176929473876953\n",
            "Train loss: 4.266761779785156\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 4.180353164672852\n",
            "Train loss: 5.4204912185668945\n",
            "Train loss: 4.745301723480225\n",
            "Train loss: 4.981489181518555\n",
            "Train loss: 4.687221527099609\n",
            "Train loss: 5.499593734741211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 628.05it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 715.15it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 718.86it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 715.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 5.008970737457275\n",
            "Train loss: 4.855337142944336\n",
            "Train loss: 4.23807954788208\n",
            "Train loss: 5.839825630187988\n",
            "Train loss: 4.866974830627441\n",
            "Train loss: 5.112966537475586\n",
            "Train loss: 5.026206016540527\n",
            "Train loss: 4.484927177429199\n",
            "Train loss: 4.996918201446533\n",
            "Train loss: 4.132694244384766\n",
            "##################################################\n",
            "Epoch: 3\n",
            "Train loss: 4.0037384033203125\n",
            "Train loss: 5.2892255783081055\n",
            "Train loss: 4.619738578796387\n",
            "Train loss: 4.8648271560668945\n",
            "Train loss: 4.561341762542725\n",
            "Train loss: 5.178956985473633\n",
            "Train loss: 4.8092780113220215\n",
            "Train loss: 4.709595203399658\n",
            "Train loss: 4.127050399780273\n",
            "Train loss: 5.623699188232422\n",
            "Train loss: 4.673162460327148\n",
            "Train loss: 4.687160491943359\n",
            "Train loss: 4.891834259033203\n",
            "Train loss: 4.351266384124756\n",
            "Train loss: 4.819890022277832\n",
            "Train loss: 4.002385139465332\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 3.831840991973877\n",
            "Train loss: 5.1598005294799805\n",
            "Train loss: 4.49549674987793\n",
            "Train loss: 4.749598503112793\n",
            "Train loss: 4.437582015991211\n",
            "Train loss: 4.86818790435791\n",
            "Train loss: 4.6143293380737305\n",
            "Train loss: 4.567535400390625\n",
            "Train loss: 4.019179344177246\n",
            "Train loss: 5.411907196044922\n",
            "Train loss: 4.486013412475586\n",
            "Train loss: 4.277170181274414\n",
            "Train loss: 4.759343147277832\n",
            "Train loss: 4.2207489013671875\n",
            "Train loss: 4.646051406860352\n",
            "Train loss: 3.8757450580596924\n",
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 3.664670944213867\n",
            "Train loss: 5.0321550369262695\n",
            "Train loss: 4.372613430023193\n",
            "Train loss: 4.635771751403809\n",
            "Train loss: 4.315884590148926\n",
            "Train loss: 4.567586898803711\n",
            "Train loss: 4.424095153808594\n",
            "Train loss: 4.428955078125\n",
            "Train loss: 3.9144978523254395\n",
            "Train loss: 5.205100059509277\n",
            "Train loss: 4.306728363037109\n",
            "Train loss: 3.8849759101867676\n",
            "Train loss: 4.628730297088623\n",
            "Train loss: 4.0933518409729\n",
            "Train loss: 4.475651741027832\n",
            "Train loss: 3.752699613571167\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53aab8a3-39e9-44ca-fab8-eca17f31ce62"
      },
      "source": [
        "skipgram.train()\r\n",
        "skipgram = skipgram.to(device)\r\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(skipgram_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = skipgram(x)  # (B, V)\r\n",
        "\r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 776.81it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 4.418552398681641\n",
            "Train loss: 4.998612403869629\n",
            "Train loss: 4.189716339111328\n",
            "Train loss: 4.50930118560791\n",
            "Train loss: 4.3430585861206055\n",
            "Train loss: 4.459402084350586\n",
            "Train loss: 4.160971164703369\n",
            "Train loss: 4.904258728027344\n",
            "Train loss: 4.015072822570801\n",
            "Train loss: 4.402087688446045\n",
            "Train loss: 4.202173709869385\n",
            "Train loss: 4.707167625427246\n",
            "Train loss: 3.858921766281128\n",
            "Train loss: 4.094736099243164\n",
            "Train loss: 5.167797088623047\n",
            "Train loss: 4.149286270141602\n",
            "Train loss: 4.834717750549316\n",
            "Train loss: 4.4451398849487305\n",
            "Train loss: 3.949012517929077\n",
            "Train loss: 4.269545078277588\n",
            "Train loss: 3.7399449348449707\n",
            "Train loss: 4.352134704589844\n",
            "Train loss: 4.5132598876953125\n",
            "Train loss: 3.43649959564209\n",
            "Train loss: 3.592538833618164\n",
            "Train loss: 4.079185485839844\n",
            "Train loss: 4.0794219970703125\n",
            "Train loss: 4.3830246925354\n",
            "Train loss: 4.401734828948975\n",
            "Train loss: 3.896967887878418\n",
            "Train loss: 4.18911075592041\n",
            "Train loss: 4.468991756439209\n",
            "Train loss: 3.940944194793701\n",
            "Train loss: 3.6932034492492676\n",
            "Train loss: 4.465372085571289\n",
            "Train loss: 4.345403671264648\n",
            "Train loss: 4.438994884490967\n",
            "Train loss: 4.3974103927612305\n",
            "Train loss: 3.9310991764068604\n",
            "Train loss: 4.2949676513671875\n",
            "Train loss: 4.364633560180664\n",
            "Train loss: 4.084460735321045\n",
            "Train loss: 4.25536584854126\n",
            "Train loss: 4.056934833526611\n",
            "Train loss: 4.284126281738281\n",
            "Train loss: 3.6710550785064697\n",
            "Train loss: 4.082463264465332\n",
            "Train loss: 4.4309821128845215\n",
            "Train loss: 3.87429141998291\n",
            "Train loss: 4.379441738128662\n",
            "Train loss: 4.011014938354492\n",
            "Train loss: 4.006112575531006\n",
            "Train loss: 4.260894775390625\n",
            "Train loss: 4.568256378173828\n",
            "Train loss: 4.434744358062744\n",
            "Train loss: 4.111639976501465\n",
            "Train loss: 4.170770168304443\n",
            "Train loss: 4.149106502532959\n",
            "Train loss: 4.333812236785889\n",
            "Train loss: 4.611817836761475\n",
            "Train loss: 4.237010955810547\n",
            "Train loss: 4.42730188369751\n",
            "Train loss: 4.071061611175537\n",
            "Train loss: 4.740606307983398\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 4.3813276290893555\n",
            "Train loss: 4.944178581237793\n",
            "Train loss: 4.151599884033203\n",
            "Train loss: 4.458186149597168\n",
            "Train loss: 4.307579517364502\n",
            "Train loss: 4.426143646240234\n",
            "Train loss: 4.125001907348633\n",
            "Train loss: 4.873416423797607\n",
            "Train loss: 3.978365421295166\n",
            "Train loss: 4.368517875671387\n",
            "Train loss: 4.1707282066345215\n",
            "Train loss: 4.680372714996338\n",
            "Train loss: 3.833078384399414\n",
            "Train loss: 4.066274642944336\n",
            "Train loss: 5.123781204223633\n",
            "Train loss: 4.121284008026123\n",
            "Train loss: 4.800938606262207\n",
            "Train loss: 4.415284156799316\n",
            "Train loss: 3.9265928268432617\n",
            "Train loss: 4.2375712394714355\n",
            "Train loss: 3.6375017166137695\n",
            "Train loss: 4.241694927215576\n",
            "Train loss: 4.457135200500488\n",
            "Train loss: 3.4113142490386963\n",
            "Train loss: 3.555185317993164\n",
            "Train loss: 4.028929710388184\n",
            "Train loss: 4.039683818817139\n",
            "Train loss: 4.349270820617676\n",
            "Train loss: 4.365349769592285\n",
            "Train loss: 3.8731327056884766\n",
            "Train loss: 4.155884265899658\n",
            "Train loss: 4.433220863342285\n",
            "Train loss: 3.9123315811157227\n",
            "Train loss: 3.6698107719421387\n",
            "Train loss: 4.433061599731445\n",
            "Train loss: 4.304298400878906\n",
            "Train loss: 4.376260757446289\n",
            "Train loss: 4.33708381652832\n",
            "Train loss: 3.90187668800354\n",
            "Train loss: 4.268118858337402\n",
            "Train loss: 4.324102878570557\n",
            "Train loss: 4.052486896514893\n",
            "Train loss: 4.173953056335449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 751.55it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 775.99it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.010544300079346\n",
            "Train loss: 4.136783599853516\n",
            "Train loss: 3.567817211151123\n",
            "Train loss: 3.9759018421173096\n",
            "Train loss: 4.376559734344482\n",
            "Train loss: 3.8455169200897217\n",
            "Train loss: 4.346393585205078\n",
            "Train loss: 3.979640483856201\n",
            "Train loss: 3.969911575317383\n",
            "Train loss: 4.227710247039795\n",
            "Train loss: 4.532289505004883\n",
            "Train loss: 4.401263236999512\n",
            "Train loss: 4.0854058265686035\n",
            "Train loss: 4.120846748352051\n",
            "Train loss: 4.1187872886657715\n",
            "Train loss: 4.306018829345703\n",
            "Train loss: 4.573902130126953\n",
            "Train loss: 4.2010345458984375\n",
            "Train loss: 4.392168045043945\n",
            "Train loss: 4.039783000946045\n",
            "Train loss: 4.695517539978027\n",
            "##################################################\n",
            "Epoch: 3\n",
            "Train loss: 4.345399856567383\n",
            "Train loss: 4.8899030685424805\n",
            "Train loss: 4.11378288269043\n",
            "Train loss: 4.407886505126953\n",
            "Train loss: 4.272377967834473\n",
            "Train loss: 4.393229007720947\n",
            "Train loss: 4.089619159698486\n",
            "Train loss: 4.842668533325195\n",
            "Train loss: 3.941926956176758\n",
            "Train loss: 4.335052490234375\n",
            "Train loss: 4.139499187469482\n",
            "Train loss: 4.653705596923828\n",
            "Train loss: 3.8076677322387695\n",
            "Train loss: 4.037975788116455\n",
            "Train loss: 5.0799970626831055\n",
            "Train loss: 4.093593597412109\n",
            "Train loss: 4.767266273498535\n",
            "Train loss: 4.385555267333984\n",
            "Train loss: 3.90439510345459\n",
            "Train loss: 4.205824851989746\n",
            "Train loss: 3.537923574447632\n",
            "Train loss: 4.134517669677734\n",
            "Train loss: 4.401528835296631\n",
            "Train loss: 3.3863840103149414\n",
            "Train loss: 3.5183663368225098\n",
            "Train loss: 3.97953724861145\n",
            "Train loss: 4.000288486480713\n",
            "Train loss: 4.316773414611816\n",
            "Train loss: 4.329385757446289\n",
            "Train loss: 3.8495421409606934\n",
            "Train loss: 4.122799873352051\n",
            "Train loss: 4.397736549377441\n",
            "Train loss: 3.8839263916015625\n",
            "Train loss: 3.6467742919921875\n",
            "Train loss: 4.400958061218262\n",
            "Train loss: 4.263584136962891\n",
            "Train loss: 4.315709590911865\n",
            "Train loss: 4.279603958129883\n",
            "Train loss: 3.8733692169189453\n",
            "Train loss: 4.241491317749023\n",
            "Train loss: 4.283883094787598\n",
            "Train loss: 4.020934581756592\n",
            "Train loss: 4.095107078552246\n",
            "Train loss: 3.9643797874450684\n",
            "Train loss: 3.993170738220215\n",
            "Train loss: 3.4676613807678223\n",
            "Train loss: 3.8728044033050537\n",
            "Train loss: 4.322661399841309\n",
            "Train loss: 3.8169214725494385\n",
            "Train loss: 4.313698768615723\n",
            "Train loss: 3.9486615657806396\n",
            "Train loss: 3.93436861038208\n",
            "Train loss: 4.19472074508667\n",
            "Train loss: 4.497466564178467\n",
            "Train loss: 4.367966651916504\n",
            "Train loss: 4.059307098388672\n",
            "Train loss: 4.071774005889893\n",
            "Train loss: 4.08877420425415\n",
            "Train loss: 4.278502464294434\n",
            "Train loss: 4.53615140914917\n",
            "Train loss: 4.1652655601501465\n",
            "Train loss: 4.357358932495117\n",
            "Train loss: 4.009739875793457\n",
            "Train loss: 4.6506733894348145\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 4.310667037963867\n",
            "Train loss: 4.835793495178223\n",
            "Train loss: 4.0762715339660645\n",
            "Train loss: 4.358417510986328\n",
            "Train loss: 4.237454414367676\n",
            "Train loss: 4.360659599304199\n",
            "Train loss: 4.054821014404297\n",
            "Train loss: 4.812016487121582\n",
            "Train loss: 3.905761957168579\n",
            "Train loss: 4.301697731018066\n",
            "Train loss: 4.108484268188477\n",
            "Train loss: 4.627166271209717\n",
            "Train loss: 3.7826905250549316\n",
            "Train loss: 4.0098395347595215\n",
            "Train loss: 5.036445140838623\n",
            "Train loss: 4.066218376159668\n",
            "Train loss: 4.733698844909668\n",
            "Train loss: 4.355953216552734\n",
            "Train loss: 3.8824191093444824\n",
            "Train loss: 4.174306869506836\n",
            "Train loss: 3.4414985179901123\n",
            "Train loss: 4.030793190002441\n",
            "Train loss: 4.346457481384277\n",
            "Train loss: 3.3617103099823\n",
            "Train loss: 3.4820919036865234\n",
            "Train loss: 3.9310224056243896\n",
            "Train loss: 3.961245059967041\n",
            "Train loss: 4.285439491271973\n",
            "Train loss: 4.293851852416992\n",
            "Train loss: 3.82619571685791\n",
            "Train loss: 4.089864730834961\n",
            "Train loss: 4.362539291381836\n",
            "Train loss: 3.8557305335998535\n",
            "Train loss: 3.624096393585205\n",
            "Train loss: 4.369058609008789\n",
            "Train loss: 4.223276615142822\n",
            "Train loss: 4.257530689239502\n",
            "Train loss: 4.22507381439209\n",
            "Train loss: 3.8455846309661865\n",
            "Train loss: 4.215085983276367\n",
            "Train loss: 4.24398136138916\n",
            "Train loss: 3.989809513092041\n",
            "Train loss: 4.018931865692139\n",
            "Train loss: 3.918445348739624\n",
            "Train loss: 3.8535566329956055\n",
            "Train loss: 3.370882272720337\n",
            "Train loss: 3.773360252380371\n",
            "Train loss: 4.269302845001221\n",
            "Train loss: 3.7885079383850098\n",
            "Train loss: 4.281360626220703\n",
            "Train loss: 3.918081760406494\n",
            "Train loss: 3.899484395980835\n",
            "Train loss: 4.161928653717041\n",
            "Train loss: 4.46369743347168\n",
            "Train loss: 4.334856986999512\n",
            "Train loss: 4.0333452224731445\n",
            "Train loss: 4.023568153381348\n",
            "Train loss: 4.059071063995361\n",
            "Train loss: 4.2512617111206055\n",
            "Train loss: 4.498569488525391\n",
            "Train loss: 4.129708290100098\n",
            "Train loss: 4.322873592376709\n",
            "Train loss: 3.9808406829833984\n",
            "Train loss: 4.6060791015625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 752.38it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 791.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 4.277043342590332\n",
            "Train loss: 4.781853199005127\n",
            "Train loss: 4.0390706062316895\n",
            "Train loss: 4.309793472290039\n",
            "Train loss: 4.202811241149902\n",
            "Train loss: 4.328438758850098\n",
            "Train loss: 4.020605087280273\n",
            "Train loss: 4.781462669372559\n",
            "Train loss: 3.869875907897949\n",
            "Train loss: 4.268455505371094\n",
            "Train loss: 4.077681541442871\n",
            "Train loss: 4.600752830505371\n",
            "Train loss: 3.758148193359375\n",
            "Train loss: 3.981867790222168\n",
            "Train loss: 4.9931254386901855\n",
            "Train loss: 4.039161682128906\n",
            "Train loss: 4.700235366821289\n",
            "Train loss: 4.326477527618408\n",
            "Train loss: 3.8606674671173096\n",
            "Train loss: 4.143021106719971\n",
            "Train loss: 3.3485209941864014\n",
            "Train loss: 3.9307150840759277\n",
            "Train loss: 4.291937828063965\n",
            "Train loss: 3.3372950553894043\n",
            "Train loss: 3.4463725090026855\n",
            "Train loss: 3.88339900970459\n",
            "Train loss: 3.9225621223449707\n",
            "Train loss: 4.255185604095459\n",
            "Train loss: 4.2587571144104\n",
            "Train loss: 3.8030920028686523\n",
            "Train loss: 4.057085990905762\n",
            "Train loss: 4.327625751495361\n",
            "Train loss: 3.8277454376220703\n",
            "Train loss: 3.601781129837036\n",
            "Train loss: 4.337361812591553\n",
            "Train loss: 4.183391571044922\n",
            "Train loss: 4.201905727386475\n",
            "Train loss: 4.17358922958374\n",
            "Train loss: 3.818528652191162\n",
            "Train loss: 4.188904762268066\n",
            "Train loss: 4.204405307769775\n",
            "Train loss: 3.9591166973114014\n",
            "Train loss: 3.9455385208129883\n",
            "Train loss: 3.8727447986602783\n",
            "Train loss: 3.718226671218872\n",
            "Train loss: 3.277768611907959\n",
            "Train loss: 3.677760362625122\n",
            "Train loss: 4.216500282287598\n",
            "Train loss: 3.7602789402008057\n",
            "Train loss: 4.249378681182861\n",
            "Train loss: 3.887903928756714\n",
            "Train loss: 3.8652584552764893\n",
            "Train loss: 4.129337787628174\n",
            "Train loss: 4.430904388427734\n",
            "Train loss: 4.301938533782959\n",
            "Train loss: 4.0075201988220215\n",
            "Train loss: 3.9762415885925293\n",
            "Train loss: 4.02968168258667\n",
            "Train loss: 4.224297046661377\n",
            "Train loss: 4.46115779876709\n",
            "Train loss: 4.094367980957031\n",
            "Train loss: 4.288713455200195\n",
            "Train loss: 3.9530067443847656\n",
            "Train loss: 4.561740875244141\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "2b016f5d-698c-436b-b96f-e06499b3c649"
      },
      "source": [
        "for word in test_words:\r\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n",
        "  emb = cbow.embedding(input_id)\r\n",
        "\r\n",
        "  print(f\"Word: {word}\")\r\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c957f5a10c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Word: {word}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df8a65a-21ae-497e-d4f6-49cc338f423b"
      },
      "source": [
        "for word in test_words:\r\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n",
        "  emb = skipgram.embedding(input_id)\r\n",
        "\r\n",
        "  print(f\"Word: {word}\")\r\n",
        "  print(max(emb.squeeze(0)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: 음식\n",
            "tensor(2.8897, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 맛\n",
            "tensor(3.1442, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 서비스\n",
            "tensor(2.4952, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 위생\n",
            "tensor(2.8184, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 가격\n",
            "tensor(2.9439, device='cuda:0', grad_fn=<UnbindBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmN4IchwisOO"
      },
      "source": [
        "!apt-get install -qq texlive texlive-xetex texlive-latex-extra pandoc\r\n",
        "!pip install -qq pypandoc\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!jupyter nbconvert --to PDF '/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}