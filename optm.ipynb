{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HZSeFsardG1NAjXBAgcMbqdG0eIpUXa4",
      "authorship_tag": "ABX9TyNolarYXznsc826tkB/URww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyun1006/colab_deep/blob/main/optm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZAXTpR6NUre"
      },
      "source": [
        "!pip install matplotlib==3.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh_TGn5acYXk"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "print(\"PyTorch version:{%s}\"%(torch.__version__))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:{%s}'%(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWgEIADpdIBZ"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzT220ZndI9F"
      },
      "source": [
        "n_data = 10000\n",
        "x_numpy = -3+6*np.random.rand(n_data,1)\n",
        "y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) + 3e-2*np.random.randn(n_data,1)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(x_numpy, y_numpy, 'r.', ms = 2)\n",
        "plt.show()\n",
        "x_torch = torch.Tensor(x_numpy).to(device)\n",
        "y_torch = torch.Tensor(y_numpy).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP73TeYseFQb"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unDRbZFzeG3P"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,name='mlp',xdim=1, hdims=[16,16],ydim=1):\n",
        "    super(Model, self).__init__()\n",
        "    self.name = name\n",
        "    self.xdim = xdim\n",
        "    self.hdims = hdims\n",
        "    self.ydim = ydim\n",
        "    self.layers = []\n",
        "    prev_hdim = self.xdim\n",
        "    for hdim in self.hdims:\n",
        "      self.layers.append(nn.Linear(prev_hdim, hdim, bias=True))\n",
        "      self.layers.append(nn.Tanh()) # activation\n",
        "      prev_hdim = hdim\n",
        "    # Final layer (wihtout activation)\n",
        "    self.layers.append(nn.Linear(prev_hdim, self.ydim, bias=True))\n",
        "\n",
        "    # concatenate all layers\n",
        "    self.net = nn.Sequential()\n",
        "    for l_idx, layer in enumerate(self.layers):\n",
        "      layer_name = \"%s_%02d\"%(type(layer).__name__.lower(), l_idx)\n",
        "      self.net.add_module(layer_name, layer)\n",
        "\n",
        "    self.init_param()\n",
        "\n",
        "  def init_param(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "print(\"Done\")\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvVbKPVegOix"
      },
      "source": [
        "LEARNING_RATE = 1e-2\n",
        "\n",
        "model_sgd = Model(name='mlp_sgd', xdim=1, hdims=[64,64], ydim=1).to(device)\n",
        "model_momentum = Model(name='mlp', xdim=1, hdims=[64,64], ydim=1).to(device)\n",
        "model_adam = Model(name='mlp', xdim=1, hdims=[64,64], ydim=1).to(device)\n",
        "\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optm_sgd = optim.SGD(model_sgd.parameters(), lr=LEARNING_RATE)\n",
        "optm_momentum = optim.SGD(model_momentum.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "optm_adam = optim.Adam(model_adam.parameters(), lr=LEARNING_RATE)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pLq8Dvbh0nf"
      },
      "source": [
        "check Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsA5Wslfh2W9"
      },
      "source": [
        "np.set_printoptions(precision=3)\n",
        "n_param = 0\n",
        "for p_idx, (param_name, param) in enumerate(model_sgd.named_parameters()):\n",
        "  if param.requires_grad:\n",
        "    param_numpy = param.detach().cpu().numpy()\n",
        "    n_param += len(param_numpy.reshape(-1))\n",
        "    print(\"[%d] name : [%s] shape:[%s].\"%(p_idx, param_name, param_numpy.shape))\n",
        "    print(\"     val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
        "print(\"Total number of parameters:[%s]\"%(format(n_param, ',d')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt73nOv-iZQ3"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GipZi0TmiaGW"
      },
      "source": [
        "MAX_ITER, BATCH_SIZE, PLOT_EVERY = 1e4, 64, 500\n",
        "\n",
        "model_sgd.init_param()\n",
        "model_momentum.init_param()\n",
        "model_adam.init_param()\n",
        "\n",
        "model_sgd.train()\n",
        "model_momentum.train()\n",
        "model_adam.train()\n",
        "\n",
        "for it in range(int(MAX_ITER)):\n",
        "  r_idx = np.random.permutation(n_data)[:BATCH_SIZE]\n",
        "  batch_x, batch_y = x_torch[r_idx], y_torch[r_idx]\n",
        "\n",
        "  y_pred_adam = model_adam.forward(batch_x)\n",
        "  loss_adam = loss(y_pred_adam, batch_y)\n",
        "  optm_adam.zero_grad()\n",
        "  loss_adam.backward()\n",
        "  optm_adam.step()\n",
        "\n",
        "  y_pred_momentum = model_momentum.forward(batch_x)\n",
        "  loss_momentum = loss(y_pred_momentum, batch_y)\n",
        "  optm_momentum.zero_grad()\n",
        "  loss_momentum.backward()\n",
        "  optm_momentum.step()\n",
        "\n",
        "  y_pred_sgd = model_sgd.forward(batch_x)\n",
        "  loss_sgd = loss(y_pred_sgd, batch_y)\n",
        "  optm_sgd.zero_grad()\n",
        "  loss_sgd.backward()\n",
        "  optm_sgd.step()\n",
        "\n",
        "\n",
        "  if ((it%PLOT_EVERY)==0) or (it==0) or (it==(MAX_ITER-1)):\n",
        "    with torch.no_grad():\n",
        "      y_sgd_numpy = model_sgd.forward(x_torch).cpu().detach().numpy()\n",
        "      y_momentum_numpy = model_momentum.forward(x_torch).cpu().detach().numpy()\n",
        "      y_adam_numpy = model_adam.forward(x_torch).cpu().detach().numpy()\n",
        "\n",
        "      plt.figure(figsize=(8,4))\n",
        "      plt.plot(x_numpy, y_numpy, 'r.', ms=4, label='GT')\n",
        "      plt.plot(x_numpy, y_sgd_numpy, 'g.', ms=2, label='SGD')\n",
        "      plt.plot(x_numpy, y_momentum_numpy, 'b.', ms=2, label=\"Momentum\")\n",
        "      plt.plot(x_numpy, y_adam_numpy, 'k.', ms=2, label=\"ADAM\")\n",
        "      plt.title(\"[%d/%d]\"%(it, MAX_ITER),fontsize=15)\n",
        "      plt.legend(labelcolor= 'linecolor', loc='upper right', fontsize=15)\n",
        "      plt.show() \n",
        "\n",
        "\n",
        "print('done')\n",
        "# 미니 배치에서 momentum은 이전의 데이터를 활용 --> 때문에, 계속 반영하기 때문에, 더 많은 데이터를 이용하는 것.\n",
        "# adam은 adaptive learning rate를 가지고 있어서, 훨씬 더 빠름.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}