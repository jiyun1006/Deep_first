{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_head_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyun1006/deeplearning-pytorch/blob/main/multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**7. Multi-head Attention**\r\n",
        "1. Multi-head attention 및 self-attention 구현.\r\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\r\n",
        "  [75, 51],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [21, 39, 10, 64, 21],\r\n",
        "  [98],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\r\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\r\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a61eb090-a85f-4fc8-ca96-19c7b9cd7648"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 38872.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0f319f-775b-4e95-c610-62a352347697"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\r\n",
        "num_heads = 8  # head의 개수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a968e21e-edce-47b9-9fea-16358ab1a8de"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 2.4761e+00, -7.2860e-01,  6.0909e-01,  ...,  3.9985e-01,\n",
            "          -7.9964e-02,  6.7592e-01],\n",
            "         [-8.6604e-01, -4.7887e-01,  3.3405e-01,  ..., -8.2642e-01,\n",
            "          -8.2004e-01,  1.4619e+00],\n",
            "         [-2.4253e-03,  6.0598e-01,  1.0527e+00,  ..., -1.4679e+00,\n",
            "           6.8125e-01, -3.7941e-01],\n",
            "         ...,\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01]],\n",
            "\n",
            "        [[-6.2930e-01,  1.4235e+00,  1.4935e+00,  ..., -6.0022e-01,\n",
            "           5.0821e-01,  6.9357e-01],\n",
            "         [ 6.1116e-01,  1.2564e+00, -6.4565e-01,  ...,  1.5727e+00,\n",
            "          -1.0037e+00,  2.4589e+00],\n",
            "         [-8.1986e-01, -1.8498e+00, -1.3489e+00,  ..., -1.4555e+00,\n",
            "          -6.7207e-02,  1.0520e-01],\n",
            "         ...,\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01]],\n",
            "\n",
            "        [[ 8.9050e-02,  3.7584e-01,  1.3978e+00,  ...,  1.2408e+00,\n",
            "          -2.6821e-01, -1.3078e+00],\n",
            "         [-2.4680e+00, -4.3777e-01, -2.0390e+00,  ..., -1.9298e-02,\n",
            "           7.3209e-02, -1.4960e+00],\n",
            "         [-1.2299e+00, -8.4934e-01, -2.6032e-01,  ...,  2.3841e-02,\n",
            "           7.8190e-01,  1.5005e-01],\n",
            "         ...,\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.5679e-02,  1.0359e+00, -1.3294e+00,  ...,  1.4405e-01,\n",
            "           3.2553e-01,  3.9045e-01],\n",
            "         [ 1.7548e-01, -1.0512e-01,  1.0316e+00,  ..., -4.9681e-01,\n",
            "           1.0380e+00, -5.5046e-01],\n",
            "         [-8.1986e-01, -1.8498e+00, -1.3489e+00,  ..., -1.4555e+00,\n",
            "          -6.7207e-02,  1.0520e-01],\n",
            "         ...,\n",
            "         [ 1.3122e+00, -5.7092e-01, -2.2806e-01,  ..., -1.2909e+00,\n",
            "           1.9049e+00,  1.7450e+00],\n",
            "         [ 1.4348e+00, -2.9018e-01,  1.4156e+00,  ...,  1.1519e+00,\n",
            "          -5.6446e-01,  1.3622e+00],\n",
            "         [-7.0400e-02,  2.8478e+00, -1.3433e+00,  ..., -3.8444e-01,\n",
            "          -4.0078e-01, -5.1995e-01]],\n",
            "\n",
            "        [[-1.0711e+00,  4.2840e-01, -2.9418e-01,  ...,  5.1137e-01,\n",
            "          -7.5131e-01, -4.6323e-01],\n",
            "         [ 1.0022e-01, -4.4960e-01,  4.4014e-01,  ..., -8.8065e-01,\n",
            "          -1.1709e+00,  9.2330e-01],\n",
            "         [-1.7016e+00, -5.3861e-01, -1.2603e+00,  ...,  8.5426e-02,\n",
            "           1.0966e+00,  1.4361e-01],\n",
            "         ...,\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01]],\n",
            "\n",
            "        [[ 3.4499e-01,  1.9063e+00, -3.1681e-01,  ..., -4.0990e-01,\n",
            "           1.1150e-02, -8.3629e-01],\n",
            "         [ 1.0022e-01, -4.4960e-01,  4.4014e-01,  ..., -8.8065e-01,\n",
            "          -1.1709e+00,  9.2330e-01],\n",
            "         [ 1.6413e+00,  3.3248e-01,  3.7338e-01,  ...,  7.4611e-01,\n",
            "          -1.1131e+00,  9.8997e-01],\n",
            "         ...,\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01],\n",
            "         [ 1.4205e+00,  2.5069e+00, -1.3622e+00,  ...,  1.2034e+00,\n",
            "          -5.0359e-01, -9.8363e-01]]], grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model) # multi head attention이 끝나고 최종적으로 합쳐주기 위한 linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782851c6-e22e-4008-e844-c5a9e0389beb"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65596fd8-d515-4b51-a5aa-3835d273f43e"
      },
      "source": [
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab88cc99-b303-463e-935e-140ad3f36b73"
      },
      "source": [
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k) 각 head가 L x d_k 개의 행렬을 가지게 되는 꼴.\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Scaled dot-product self-attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce9eecf-b458-4e31-f015-04ca3bec3c5f"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.0517, 0.0424, 0.0419,  ..., 0.0877, 0.0877, 0.0877],\n",
            "          [0.0289, 0.0506, 0.0353,  ..., 0.0510, 0.0510, 0.0510],\n",
            "          [0.0750, 0.0927, 0.0622,  ..., 0.0343, 0.0343, 0.0343],\n",
            "          ...,\n",
            "          [0.0643, 0.0409, 0.0585,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0643, 0.0409, 0.0585,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0643, 0.0409, 0.0585,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         [[0.0556, 0.0473, 0.0396,  ..., 0.0671, 0.0671, 0.0671],\n",
            "          [0.0564, 0.0414, 0.0341,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          [0.0512, 0.0475, 0.0431,  ..., 0.0638, 0.0638, 0.0638],\n",
            "          ...,\n",
            "          [0.0568, 0.0541, 0.0358,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0568, 0.0541, 0.0358,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0568, 0.0541, 0.0358,  ..., 0.0442, 0.0442, 0.0442]],\n",
            "\n",
            "         [[0.0437, 0.0581, 0.0405,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          [0.0508, 0.0783, 0.0448,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0363, 0.0614, 0.0407,  ..., 0.0440, 0.0440, 0.0440],\n",
            "          ...,\n",
            "          [0.0639, 0.0347, 0.0354,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0639, 0.0347, 0.0354,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0639, 0.0347, 0.0354,  ..., 0.0584, 0.0584, 0.0584]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0224, 0.0325, 0.0639,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0365, 0.0550, 0.0707,  ..., 0.0315, 0.0315, 0.0315],\n",
            "          [0.0524, 0.0592, 0.0623,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          ...,\n",
            "          [0.0543, 0.0434, 0.0476,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0543, 0.0434, 0.0476,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0543, 0.0434, 0.0476,  ..., 0.0403, 0.0403, 0.0403]],\n",
            "\n",
            "         [[0.0546, 0.0639, 0.0386,  ..., 0.0581, 0.0581, 0.0581],\n",
            "          [0.0455, 0.0558, 0.0478,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          [0.0813, 0.0359, 0.0716,  ..., 0.0368, 0.0368, 0.0368],\n",
            "          ...,\n",
            "          [0.0434, 0.0981, 0.0627,  ..., 0.0380, 0.0380, 0.0380],\n",
            "          [0.0434, 0.0981, 0.0627,  ..., 0.0380, 0.0380, 0.0380],\n",
            "          [0.0434, 0.0981, 0.0627,  ..., 0.0380, 0.0380, 0.0380]],\n",
            "\n",
            "         [[0.0450, 0.0458, 0.0312,  ..., 0.0301, 0.0301, 0.0301],\n",
            "          [0.0624, 0.0229, 0.0571,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          [0.0325, 0.0422, 0.0596,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          ...,\n",
            "          [0.0615, 0.0612, 0.0555,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          [0.0615, 0.0612, 0.0555,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          [0.0615, 0.0612, 0.0555,  ..., 0.0591, 0.0591, 0.0591]]],\n",
            "\n",
            "\n",
            "        [[[0.0431, 0.0501, 0.0374,  ..., 0.0516, 0.0516, 0.0516],\n",
            "          [0.0599, 0.0477, 0.0369,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0574, 0.0680, 0.0432,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          ...,\n",
            "          [0.0506, 0.0670, 0.0285,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0506, 0.0670, 0.0285,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0506, 0.0670, 0.0285,  ..., 0.0492, 0.0492, 0.0492]],\n",
            "\n",
            "         [[0.0517, 0.0820, 0.0404,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0427, 0.0790, 0.0343,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0150, 0.0430, 0.0494,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0481, 0.0671, 0.0438,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0481, 0.0671, 0.0438,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0481, 0.0671, 0.0438,  ..., 0.0476, 0.0476, 0.0476]],\n",
            "\n",
            "         [[0.0609, 0.0402, 0.0561,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0626, 0.0926, 0.0744,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0394, 0.0260, 0.0402,  ..., 0.0557, 0.0557, 0.0557],\n",
            "          ...,\n",
            "          [0.0372, 0.0688, 0.0458,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0372, 0.0688, 0.0458,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0372, 0.0688, 0.0458,  ..., 0.0480, 0.0480, 0.0480]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0284, 0.0672, 0.0361,  ..., 0.0510, 0.0510, 0.0510],\n",
            "          [0.0339, 0.0461, 0.0177,  ..., 0.0548, 0.0548, 0.0548],\n",
            "          [0.0492, 0.0536, 0.1194,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          ...,\n",
            "          [0.0843, 0.0567, 0.0662,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0843, 0.0567, 0.0662,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0843, 0.0567, 0.0662,  ..., 0.0449, 0.0449, 0.0449]],\n",
            "\n",
            "         [[0.0305, 0.0422, 0.0578,  ..., 0.0526, 0.0526, 0.0526],\n",
            "          [0.0488, 0.0995, 0.0471,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0725, 0.0462, 0.0712,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          ...,\n",
            "          [0.1283, 0.0483, 0.0356,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.1283, 0.0483, 0.0356,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.1283, 0.0483, 0.0356,  ..., 0.0452, 0.0452, 0.0452]],\n",
            "\n",
            "         [[0.1125, 0.0359, 0.0598,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0328, 0.0384, 0.0382,  ..., 0.0548, 0.0548, 0.0548],\n",
            "          [0.0169, 0.0396, 0.0205,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          ...,\n",
            "          [0.0360, 0.0468, 0.0425,  ..., 0.0536, 0.0536, 0.0536],\n",
            "          [0.0360, 0.0468, 0.0425,  ..., 0.0536, 0.0536, 0.0536],\n",
            "          [0.0360, 0.0468, 0.0425,  ..., 0.0536, 0.0536, 0.0536]]],\n",
            "\n",
            "\n",
            "        [[[0.0620, 0.0333, 0.0801,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0200, 0.0378, 0.0764,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0622, 0.0536, 0.0704,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          ...,\n",
            "          [0.0663, 0.0278, 0.0681,  ..., 0.0509, 0.0509, 0.0509],\n",
            "          [0.0663, 0.0278, 0.0681,  ..., 0.0509, 0.0509, 0.0509],\n",
            "          [0.0663, 0.0278, 0.0681,  ..., 0.0509, 0.0509, 0.0509]],\n",
            "\n",
            "         [[0.0407, 0.0780, 0.0388,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0578, 0.0419, 0.0522,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          [0.0557, 0.0329, 0.0366,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          ...,\n",
            "          [0.0475, 0.0666, 0.0430,  ..., 0.0431, 0.0431, 0.0431],\n",
            "          [0.0475, 0.0666, 0.0430,  ..., 0.0431, 0.0431, 0.0431],\n",
            "          [0.0475, 0.0666, 0.0430,  ..., 0.0431, 0.0431, 0.0431]],\n",
            "\n",
            "         [[0.0670, 0.0570, 0.0469,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0744, 0.0833, 0.0708,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0452, 0.0311, 0.0511,  ..., 0.0656, 0.0656, 0.0656],\n",
            "          ...,\n",
            "          [0.0521, 0.0230, 0.0343,  ..., 0.0537, 0.0537, 0.0537],\n",
            "          [0.0521, 0.0230, 0.0343,  ..., 0.0537, 0.0537, 0.0537],\n",
            "          [0.0521, 0.0230, 0.0343,  ..., 0.0537, 0.0537, 0.0537]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0541, 0.0573, 0.0534,  ..., 0.0418, 0.0418, 0.0418],\n",
            "          [0.0245, 0.0370, 0.0339,  ..., 0.0606, 0.0606, 0.0606],\n",
            "          [0.0722, 0.0436, 0.0276,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          ...,\n",
            "          [0.0807, 0.0573, 0.0584,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0807, 0.0573, 0.0584,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0807, 0.0573, 0.0584,  ..., 0.0404, 0.0404, 0.0404]],\n",
            "\n",
            "         [[0.0294, 0.0496, 0.0392,  ..., 0.0612, 0.0612, 0.0612],\n",
            "          [0.0418, 0.0410, 0.0568,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0458, 0.0755, 0.0425,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          ...,\n",
            "          [0.0905, 0.0447, 0.0500,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0905, 0.0447, 0.0500,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0905, 0.0447, 0.0500,  ..., 0.0450, 0.0450, 0.0450]],\n",
            "\n",
            "         [[0.0697, 0.0510, 0.0466,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          [0.0471, 0.0525, 0.0409,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          [0.0418, 0.0561, 0.0185,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          ...,\n",
            "          [0.0246, 0.0233, 0.0357,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0246, 0.0233, 0.0357,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0246, 0.0233, 0.0357,  ..., 0.0573, 0.0573, 0.0573]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0436, 0.0563, 0.0243,  ..., 0.0300, 0.0333, 0.0436],\n",
            "          [0.0400, 0.0478, 0.0969,  ..., 0.0874, 0.0452, 0.0366],\n",
            "          [0.0423, 0.0593, 0.0466,  ..., 0.0810, 0.0415, 0.0422],\n",
            "          ...,\n",
            "          [0.0542, 0.0649, 0.0402,  ..., 0.0296, 0.0593, 0.0293],\n",
            "          [0.0634, 0.0822, 0.0500,  ..., 0.0494, 0.0293, 0.0507],\n",
            "          [0.0702, 0.0385, 0.0351,  ..., 0.0406, 0.0262, 0.0358]],\n",
            "\n",
            "         [[0.0592, 0.0286, 0.0444,  ..., 0.0503, 0.0793, 0.0343],\n",
            "          [0.0621, 0.0473, 0.0395,  ..., 0.0384, 0.0452, 0.0529],\n",
            "          [0.0531, 0.0282, 0.0669,  ..., 0.0577, 0.0386, 0.0415],\n",
            "          ...,\n",
            "          [0.0522, 0.0473, 0.0451,  ..., 0.0417, 0.0489, 0.0566],\n",
            "          [0.0510, 0.0560, 0.0608,  ..., 0.0473, 0.0331, 0.0378],\n",
            "          [0.0395, 0.0549, 0.0263,  ..., 0.0671, 0.0448, 0.0528]],\n",
            "\n",
            "         [[0.0422, 0.0777, 0.0620,  ..., 0.0542, 0.0378, 0.0344],\n",
            "          [0.0271, 0.0213, 0.0448,  ..., 0.0373, 0.0280, 0.0873],\n",
            "          [0.0509, 0.0331, 0.0604,  ..., 0.0726, 0.0341, 0.0340],\n",
            "          ...,\n",
            "          [0.0447, 0.0529, 0.0357,  ..., 0.0681, 0.0277, 0.0367],\n",
            "          [0.0384, 0.0523, 0.0724,  ..., 0.0366, 0.0409, 0.0505],\n",
            "          [0.0398, 0.0263, 0.0313,  ..., 0.0342, 0.0284, 0.0297]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0450, 0.0524, 0.0284,  ..., 0.0915, 0.0705, 0.0736],\n",
            "          [0.0497, 0.0461, 0.0304,  ..., 0.0792, 0.0389, 0.0243],\n",
            "          [0.0371, 0.0327, 0.1159,  ..., 0.0508, 0.0588, 0.0333],\n",
            "          ...,\n",
            "          [0.0454, 0.0621, 0.0570,  ..., 0.0372, 0.0392, 0.0621],\n",
            "          [0.0478, 0.0457, 0.0286,  ..., 0.0315, 0.0628, 0.0347],\n",
            "          [0.0420, 0.0358, 0.0798,  ..., 0.0270, 0.0398, 0.0610]],\n",
            "\n",
            "         [[0.0338, 0.0344, 0.0443,  ..., 0.0563, 0.0310, 0.0684],\n",
            "          [0.0547, 0.0493, 0.0525,  ..., 0.0346, 0.0618, 0.0715],\n",
            "          [0.0776, 0.0362, 0.0601,  ..., 0.0554, 0.0922, 0.0467],\n",
            "          ...,\n",
            "          [0.0524, 0.0530, 0.0496,  ..., 0.0373, 0.0740, 0.0514],\n",
            "          [0.0541, 0.0457, 0.0282,  ..., 0.0412, 0.0594, 0.0461],\n",
            "          [0.0426, 0.0298, 0.0554,  ..., 0.0667, 0.0779, 0.0381]],\n",
            "\n",
            "         [[0.0391, 0.0401, 0.0701,  ..., 0.0354, 0.0369, 0.0421],\n",
            "          [0.0435, 0.0465, 0.0298,  ..., 0.0502, 0.0540, 0.0337],\n",
            "          [0.0622, 0.0373, 0.0339,  ..., 0.0296, 0.0505, 0.0444],\n",
            "          ...,\n",
            "          [0.0484, 0.0367, 0.0436,  ..., 0.0478, 0.0317, 0.0385],\n",
            "          [0.0399, 0.0317, 0.0269,  ..., 0.0444, 0.0251, 0.0587],\n",
            "          [0.0309, 0.0468, 0.0365,  ..., 0.0326, 0.0749, 0.0451]]],\n",
            "\n",
            "\n",
            "        [[[0.0496, 0.0519, 0.0319,  ..., 0.0719, 0.0719, 0.0719],\n",
            "          [0.0529, 0.0542, 0.0481,  ..., 0.0297, 0.0297, 0.0297],\n",
            "          [0.0693, 0.0446, 0.0310,  ..., 0.0649, 0.0649, 0.0649],\n",
            "          ...,\n",
            "          [0.0254, 0.0536, 0.0589,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0254, 0.0536, 0.0589,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0254, 0.0536, 0.0589,  ..., 0.0492, 0.0492, 0.0492]],\n",
            "\n",
            "         [[0.0395, 0.0374, 0.0325,  ..., 0.0705, 0.0705, 0.0705],\n",
            "          [0.0555, 0.0497, 0.0726,  ..., 0.0377, 0.0377, 0.0377],\n",
            "          [0.0405, 0.0597, 0.0463,  ..., 0.0723, 0.0723, 0.0723],\n",
            "          ...,\n",
            "          [0.0393, 0.0680, 0.0378,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0393, 0.0680, 0.0378,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0393, 0.0680, 0.0378,  ..., 0.0442, 0.0442, 0.0442]],\n",
            "\n",
            "         [[0.0609, 0.0726, 0.0498,  ..., 0.0574, 0.0574, 0.0574],\n",
            "          [0.0449, 0.0385, 0.0317,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0651, 0.0355, 0.0707,  ..., 0.0374, 0.0374, 0.0374],\n",
            "          ...,\n",
            "          [0.0454, 0.0398, 0.0599,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0454, 0.0398, 0.0599,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0454, 0.0398, 0.0599,  ..., 0.0518, 0.0518, 0.0518]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0524, 0.0842, 0.0475,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0368, 0.0256, 0.0628,  ..., 0.0627, 0.0627, 0.0627],\n",
            "          [0.0577, 0.1080, 0.0450,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          ...,\n",
            "          [0.0695, 0.0335, 0.0275,  ..., 0.0396, 0.0396, 0.0396],\n",
            "          [0.0695, 0.0335, 0.0275,  ..., 0.0396, 0.0396, 0.0396],\n",
            "          [0.0695, 0.0335, 0.0275,  ..., 0.0396, 0.0396, 0.0396]],\n",
            "\n",
            "         [[0.0621, 0.0517, 0.0483,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0297, 0.0357, 0.0648,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0437, 0.0423, 0.0426,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          ...,\n",
            "          [0.0362, 0.0567, 0.0498,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0362, 0.0567, 0.0498,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0362, 0.0567, 0.0498,  ..., 0.0405, 0.0405, 0.0405]],\n",
            "\n",
            "         [[0.0754, 0.0523, 0.0498,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0472, 0.0482, 0.0425,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0503, 0.0329, 0.0902,  ..., 0.0350, 0.0350, 0.0350],\n",
            "          ...,\n",
            "          [0.0344, 0.0361, 0.0573,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0344, 0.0361, 0.0573,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0344, 0.0361, 0.0573,  ..., 0.0518, 0.0518, 0.0518]]],\n",
            "\n",
            "\n",
            "        [[[0.0504, 0.0443, 0.0715,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          [0.0586, 0.0588, 0.0796,  ..., 0.0322, 0.0322, 0.0322],\n",
            "          [0.0572, 0.0491, 0.0454,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          ...,\n",
            "          [0.0657, 0.0509, 0.0423,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0657, 0.0509, 0.0423,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0657, 0.0509, 0.0423,  ..., 0.0467, 0.0467, 0.0467]],\n",
            "\n",
            "         [[0.0344, 0.0598, 0.0520,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0514, 0.0494, 0.0903,  ..., 0.0375, 0.0375, 0.0375],\n",
            "          [0.0597, 0.0721, 0.0519,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          ...,\n",
            "          [0.0447, 0.0624, 0.0540,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0447, 0.0624, 0.0540,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0447, 0.0624, 0.0540,  ..., 0.0405, 0.0405, 0.0405]],\n",
            "\n",
            "         [[0.0629, 0.0245, 0.0426,  ..., 0.0269, 0.0269, 0.0269],\n",
            "          [0.0762, 0.0401, 0.0449,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          [0.0604, 0.0566, 0.0547,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          ...,\n",
            "          [0.0511, 0.0448, 0.0495,  ..., 0.0582, 0.0582, 0.0582],\n",
            "          [0.0511, 0.0448, 0.0495,  ..., 0.0582, 0.0582, 0.0582],\n",
            "          [0.0511, 0.0448, 0.0495,  ..., 0.0582, 0.0582, 0.0582]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0491, 0.0427, 0.0691,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          [0.0379, 0.0250, 0.0390,  ..., 0.0613, 0.0613, 0.0613],\n",
            "          [0.0487, 0.0412, 0.0434,  ..., 0.0670, 0.0670, 0.0670],\n",
            "          ...,\n",
            "          [0.0476, 0.0327, 0.0393,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          [0.0476, 0.0327, 0.0393,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          [0.0476, 0.0327, 0.0393,  ..., 0.0387, 0.0387, 0.0387]],\n",
            "\n",
            "         [[0.0677, 0.0323, 0.0439,  ..., 0.0535, 0.0535, 0.0535],\n",
            "          [0.0372, 0.0415, 0.0413,  ..., 0.0585, 0.0585, 0.0585],\n",
            "          [0.0479, 0.0544, 0.0724,  ..., 0.0368, 0.0368, 0.0368],\n",
            "          ...,\n",
            "          [0.0327, 0.0659, 0.0498,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0327, 0.0659, 0.0498,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0327, 0.0659, 0.0498,  ..., 0.0470, 0.0470, 0.0470]],\n",
            "\n",
            "         [[0.0335, 0.0382, 0.0341,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0516, 0.0453, 0.0439,  ..., 0.0528, 0.0528, 0.0528],\n",
            "          [0.0442, 0.0476, 0.0640,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          ...,\n",
            "          [0.0480, 0.0401, 0.0447,  ..., 0.0575, 0.0575, 0.0575],\n",
            "          [0.0480, 0.0401, 0.0447,  ..., 0.0575, 0.0575, 0.0575],\n",
            "          [0.0480, 0.0401, 0.0447,  ..., 0.0575, 0.0575, 0.0575]]]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25i5cRSEu7F"
      },
      "source": [
        "가중합 구하는 부분\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5238f4e-a48c-443c-d495-8c7a233e632f"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k) 가중 합 구하는 부분\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### **각 head의 결과물 병합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear transformation합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6e84c9-1fdf-49ab-a32f-ed24dfb92d5c"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\r\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bc63cb-0d55-48a3-ce48-9d2a5c9614e5"
      },
      "source": [
        "outputs = w_0(attn_values)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1122,  0.0884,  0.0375,  ...,  0.1817,  0.0401,  0.0583],\n",
            "         [ 0.0847,  0.0543,  0.1429,  ...,  0.1294,  0.0264,  0.0867],\n",
            "         [ 0.0859,  0.0524,  0.1587,  ...,  0.1333,  0.0111,  0.0725],\n",
            "         ...,\n",
            "         [ 0.0919,  0.1168,  0.1214,  ...,  0.1824,  0.0553, -0.0015],\n",
            "         [ 0.0919,  0.1168,  0.1214,  ...,  0.1824,  0.0553, -0.0015],\n",
            "         [ 0.0919,  0.1168,  0.1214,  ...,  0.1824,  0.0553, -0.0015]],\n",
            "\n",
            "        [[ 0.4535,  0.3756, -0.3434,  ...,  0.3122,  0.3434, -0.0211],\n",
            "         [ 0.4135,  0.4138, -0.3151,  ...,  0.3449,  0.2774, -0.0910],\n",
            "         [ 0.4243,  0.4217, -0.3060,  ...,  0.3528,  0.3420, -0.0577],\n",
            "         ...,\n",
            "         [ 0.3870,  0.4032, -0.2868,  ...,  0.3416,  0.3164, -0.0681],\n",
            "         [ 0.3870,  0.4032, -0.2868,  ...,  0.3416,  0.3164, -0.0681],\n",
            "         [ 0.3870,  0.4032, -0.2868,  ...,  0.3416,  0.3164, -0.0681]],\n",
            "\n",
            "        [[ 0.2257,  0.3273, -0.1103,  ...,  0.2016,  0.2097, -0.1306],\n",
            "         [ 0.2509,  0.2792, -0.1773,  ...,  0.2475,  0.1459, -0.1100],\n",
            "         [ 0.2247,  0.3307, -0.1208,  ...,  0.1532,  0.2706, -0.1345],\n",
            "         ...,\n",
            "         [ 0.1876,  0.2909, -0.1024,  ...,  0.1505,  0.2310, -0.1819],\n",
            "         [ 0.1876,  0.2909, -0.1024,  ...,  0.1505,  0.2310, -0.1819],\n",
            "         [ 0.1876,  0.2909, -0.1024,  ...,  0.1505,  0.2310, -0.1819]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0290,  0.1109,  0.0458,  ...,  0.0615,  0.0975, -0.1319],\n",
            "         [-0.0415,  0.0540,  0.0751,  ...,  0.0040,  0.0841, -0.1410],\n",
            "         [-0.0634,  0.0387,  0.0547,  ...,  0.0343,  0.1359, -0.0978],\n",
            "         ...,\n",
            "         [-0.0425,  0.0918,  0.0432,  ..., -0.0078,  0.0906, -0.0981],\n",
            "         [-0.0244,  0.0692,  0.0169,  ...,  0.0163,  0.1140, -0.1405],\n",
            "         [-0.0469,  0.1140,  0.0772,  ...,  0.0175,  0.1295, -0.0297]],\n",
            "\n",
            "        [[ 0.1140,  0.1231, -0.0389,  ...,  0.1759,  0.1128, -0.0774],\n",
            "         [ 0.1148,  0.1017, -0.0173,  ...,  0.1715,  0.0603, -0.0293],\n",
            "         [ 0.0952,  0.0978, -0.0600,  ...,  0.1441,  0.1005, -0.0766],\n",
            "         ...,\n",
            "         [ 0.0633,  0.1067, -0.0486,  ...,  0.1757,  0.0861, -0.1321],\n",
            "         [ 0.0633,  0.1067, -0.0486,  ...,  0.1757,  0.0861, -0.1321],\n",
            "         [ 0.0633,  0.1067, -0.0486,  ...,  0.1757,  0.0861, -0.1321]],\n",
            "\n",
            "        [[ 0.2116,  0.2101, -0.0621,  ...,  0.1190,  0.1306, -0.0630],\n",
            "         [ 0.2698,  0.2394, -0.0660,  ...,  0.1059,  0.1664,  0.0046],\n",
            "         [ 0.2350,  0.1965, -0.1238,  ...,  0.0703,  0.1394, -0.0025],\n",
            "         ...,\n",
            "         [ 0.1996,  0.2238, -0.0815,  ...,  0.0964,  0.1703, -0.0547],\n",
            "         [ 0.1996,  0.2238, -0.0815,  ...,  0.0964,  0.1703, -0.0547],\n",
            "         [ 0.1996,  0.2238, -0.0815,  ...,  0.0964,  0.1703, -0.0547]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  def self_attention(self, q, k, v):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8418fd66-fe6f-4625-95b9-9df71e46b708"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 2.8643e-03,  1.1555e-01,  1.2496e-01,  ..., -3.7488e-02,\n",
            "          -3.0124e-02,  3.1039e-02],\n",
            "         [-1.4078e-02,  1.0972e-01,  1.0326e-01,  ..., -6.7813e-02,\n",
            "          -5.5103e-02,  6.8791e-02],\n",
            "         [-2.8697e-02,  6.5568e-02,  1.0969e-01,  ..., -8.0120e-02,\n",
            "          -1.6853e-02,  1.4531e-02],\n",
            "         ...,\n",
            "         [ 1.7992e-02,  1.0360e-01,  6.1830e-02,  ..., -1.1121e-01,\n",
            "          -4.9953e-02,  8.3009e-03],\n",
            "         [ 1.7992e-02,  1.0360e-01,  6.1830e-02,  ..., -1.1121e-01,\n",
            "          -4.9953e-02,  8.3010e-03],\n",
            "         [ 1.7992e-02,  1.0360e-01,  6.1830e-02,  ..., -1.1121e-01,\n",
            "          -4.9953e-02,  8.3010e-03]],\n",
            "\n",
            "        [[-1.9409e-01, -1.6303e-01,  8.1865e-02,  ..., -3.5009e-01,\n",
            "          -2.2865e-01, -5.5588e-03],\n",
            "         [-1.8015e-01, -1.0694e-01,  1.1501e-01,  ..., -3.4100e-01,\n",
            "          -3.0557e-01,  2.6842e-02],\n",
            "         [-1.8776e-01, -1.3219e-01,  1.0427e-01,  ..., -3.7695e-01,\n",
            "          -2.3110e-01,  6.5802e-02],\n",
            "         ...,\n",
            "         [-1.7091e-01, -1.3518e-01,  7.6754e-02,  ..., -3.3565e-01,\n",
            "          -3.1471e-01,  1.0806e-02],\n",
            "         [-1.7091e-01, -1.3518e-01,  7.6754e-02,  ..., -3.3565e-01,\n",
            "          -3.1471e-01,  1.0806e-02],\n",
            "         [-1.7091e-01, -1.3518e-01,  7.6754e-02,  ..., -3.3565e-01,\n",
            "          -3.1471e-01,  1.0806e-02]],\n",
            "\n",
            "        [[-1.2240e-01, -8.1718e-02,  1.2985e-01,  ..., -1.2778e-01,\n",
            "          -9.4506e-02, -2.5824e-03],\n",
            "         [-1.0761e-01, -1.0372e-02,  7.6034e-02,  ..., -1.7901e-01,\n",
            "          -5.6435e-02,  3.0685e-02],\n",
            "         [-1.2099e-01, -8.0590e-02,  1.0754e-01,  ..., -2.4162e-01,\n",
            "          -8.0017e-02,  5.8508e-02],\n",
            "         ...,\n",
            "         [-3.4437e-02, -8.0689e-02,  6.0898e-02,  ..., -2.0529e-01,\n",
            "          -1.4903e-01,  6.0502e-02],\n",
            "         [-3.4437e-02, -8.0689e-02,  6.0898e-02,  ..., -2.0529e-01,\n",
            "          -1.4903e-01,  6.0502e-02],\n",
            "         [-3.4437e-02, -8.0689e-02,  6.0898e-02,  ..., -2.0529e-01,\n",
            "          -1.4903e-01,  6.0502e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.6801e-02,  1.4469e-02,  4.0561e-02,  ...,  5.7329e-02,\n",
            "           1.2053e-02, -1.7245e-01],\n",
            "         [-6.9444e-02,  3.8273e-02,  3.4853e-02,  ...,  6.8765e-02,\n",
            "          -9.1786e-02, -9.0049e-02],\n",
            "         [-9.9883e-02, -5.7184e-04,  3.6711e-02,  ...,  4.3136e-02,\n",
            "           1.9958e-04, -1.2243e-01],\n",
            "         ...,\n",
            "         [-8.4137e-02, -6.0723e-02,  3.8828e-02,  ...,  7.1392e-02,\n",
            "          -6.2897e-02, -7.1810e-02],\n",
            "         [-7.1533e-02, -3.9160e-02,  5.9100e-02,  ...,  7.8284e-02,\n",
            "          -1.5090e-02, -5.4300e-02],\n",
            "         [-7.4022e-02, -1.6865e-02,  1.1235e-02,  ...,  2.2578e-02,\n",
            "          -6.3534e-03, -1.1426e-01]],\n",
            "\n",
            "        [[-3.8991e-02,  5.7158e-02,  1.9260e-02,  ..., -1.9603e-01,\n",
            "          -9.7644e-02, -5.4798e-02],\n",
            "         [-1.2775e-02,  4.8686e-02,  2.8838e-02,  ..., -2.8531e-01,\n",
            "          -9.3781e-02, -5.6003e-02],\n",
            "         [-2.0497e-02,  9.7728e-02, -2.9277e-02,  ..., -2.3449e-01,\n",
            "          -6.4507e-03, -3.8425e-02],\n",
            "         ...,\n",
            "         [ 1.1617e-02,  1.0468e-01, -2.6169e-02,  ..., -1.9521e-01,\n",
            "          -1.1393e-01, -1.2875e-02],\n",
            "         [ 1.1617e-02,  1.0468e-01, -2.6169e-02,  ..., -1.9521e-01,\n",
            "          -1.1393e-01, -1.2875e-02],\n",
            "         [ 1.1617e-02,  1.0468e-01, -2.6169e-02,  ..., -1.9521e-01,\n",
            "          -1.1393e-01, -1.2875e-02]],\n",
            "\n",
            "        [[-1.4393e-03,  1.0670e-01, -3.5127e-02,  ..., -2.0206e-01,\n",
            "           6.7983e-02,  2.3542e-02],\n",
            "         [-2.4730e-02,  7.5943e-02,  3.7259e-02,  ..., -2.1099e-01,\n",
            "           1.6493e-02,  3.6061e-03],\n",
            "         [-5.2394e-03,  4.2587e-02,  4.7808e-02,  ..., -1.9942e-01,\n",
            "           1.5732e-02,  2.0320e-02],\n",
            "         ...,\n",
            "         [ 8.8007e-03,  6.1124e-02,  1.2385e-02,  ..., -2.0334e-01,\n",
            "           5.4914e-04,  4.8777e-03],\n",
            "         [ 8.8007e-03,  6.1124e-02,  1.2385e-02,  ..., -2.0334e-01,\n",
            "           5.4910e-04,  4.8777e-03],\n",
            "         [ 8.8007e-03,  6.1124e-02,  1.2385e-02,  ..., -2.0334e-01,\n",
            "           5.4910e-04,  4.8777e-03]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}